name: Run F1 Scraper and API

on:
  push:
    branches: [main]
  workflow_dispatch:  # allows manual triggering

jobs:
  build:

    runs-on: ubuntu-latest

    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Debug environment variables
      run: |
        echo "SUPABASE_URL is $SUPABASE_URL"
        echo "SUPABASE_SERVICE_ROLE_KEY is $SUPABASE_SERVICE_ROLE_KEY"

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt

    - name: Install Chromium and ChromeDriver
      run: |
        sudo apt-get update
        sudo apt-get install -y chromium-browser chromium-chromedriver
        sudo apt-get install -y --no-install-recommends chromium-browser chromium-chromedriver

    - name: Check Chromium and ChromeDriver locations
      run: |
        which chromium-browser
        which chromedriver

    - name: Run scraper (prints WDC and WCC)
      run: |
        python -c "from F1Scaper import getWDC, getWCC; print('WDC:', getWDC()); print('WCC:', getWCC())"

    # - name: Print next race
    #   run: |
    #     python -c "from F1Scraper import nextrace; print('Next Race:', nextrace())"

    # - name: Print driver photos 
    #   run: |
    #     python -c "from F1Scraper import getDriverPhotos; photos= getDriverPhotos(); print('Driver Photos:', photos)"

    - name: Save scraped data to Supabase
      run: python save_data.py